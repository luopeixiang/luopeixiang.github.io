---
layout:     post
title:      "一些统计学习基础概念汇总(不定时更新...)"
subtitle:   ""
date:       2018-08-26
author:     "MaggicQ"
header-img: "img/post-bg-universe.jpg"
tags:
    - machine_learning
    - 数学
    - 统计学习
---

**本文目录**
* TOC
{:toc}



###  生成式模型与判别式模型

生成式模型与判别式模型：

1. **判别式模型的工作过程**，他们直接将数据的Y（或者label），根据所提供的features，学习，最后画出了一个明显或者比较明显的边界（具体怎么做到的？通过复杂的函数映射，或者决策叠加等等mechanism），这一点线性LR、线性SVM应该很明显吧。 **判别式模型的特点**归纳如下：

   1. 直接对条件概率分布$P(Y \| X)$进行建模
   2. 对所有样本只构建一个模型，确认总体判别边界
   3. 观测到输入什么特征，就预测最可能的label
   4. 优点是对数据量要求没有生成式的严格，速度比较快，小数据量下准确率也会比较好

2. **生成式模型的工作过程**，他们先从训练样本数据中，将所有的数据的分布情况摸透，然后最终确定一个分布，来作为我的所有的输入数据的分布，并且他是一个联合分布 $P(X,Y)$ (注意 $X$ 包含所有的特征 $x_{i}$ ， $Y$ 包含所有的label)。然后我来了新的样本数据（inference），好，通过学习来的模型的联合分布 $P(X,Y)$ ，再结合新样本给的 $X$ ，通过条件概率就能出来 $Y$ ：
   
$$

   P(Y | X) = \frac{P(X, Y)}{P(X)}
   
$$

   **生成式的模型特点**归纳如下：

   1. 对联合概率分布$P(X, Y)$建模
   2. 要对每个label都建模，最终选择最优概率label作为结果，没有什么判别边界
   3. 中间生成联合分布，并可生成采样数据
   4. 有点在于，所包含的信息齐全；生成式模型关注结果是如何产生的，但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，速度也比较慢。






### 奥卡姆剃刀原则 

奥卡姆剃刀原则应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型。





### 极大似然估计与极大后验概率估计

极大似然估计与极大后验概率估计是统计中两种对模型的参数确定的方法，两种参数估计方法使用不同的思想。前者来自于**频率派**，认为**参数是固定的**，我们要做的事情就是根据已经掌握的数据来估计这个参数；而后者属于**贝叶斯派**，认为**参数也是服从某种概率分布的**，已有的数据只是在这种参数的分布下产生的。所以，直观理解上，极大似然估计就是假设一个参数 $\theta$，然后根据数据来求出这个$\theta$，而贝叶斯估计的难点在于$p(\theta)$需要人为设定，之后再考虑结合最大后验估计**MAP**（maximum a posterior）方法来求一个具体的$\theta$。



#### 极大似然估计

假设已有的数据为$D$，要估计的参数为$\theta$， 那么极大似然估计就是要在数据给定的情况下，找到一个最优参数$\theta^*$，使得这些数据$D$出现的概率最大：

$$

\theta^* = \arg \max_{\theta}p(D | \theta )

$$

这里面$p(D \| \theta)$就是似然函数，我们要做的就是求一个是似然最大的参数，所以称为极大似然估计。 
想求解这个问题，需要假设我们的数据是相互独立的。$D=\{x_1,x_2,x_3,...,x_n\}$，这时候有：

$$

p(D|\theta)=\prod_{i=1}^{n}p(x_i|\theta)

$$



一般对上式取对数求解对数极大似然，就可以把连乘变成求和，然后求导取极值点就是要求的参数值。



#### 极大后验概率估计（MAP）

先明确几个概念：

**先验概率与后验概率**：后验概率是指掌握了一定量的数据后我们的参数分布是怎么样的，表示为$p(θ\|D)$；那先验概率就是在没有掌握数据后我们的参数怎么分布。



相比起极大似然估计，极大后验概率估计是更贴合贝叶斯学派思想的做法；也有不少人直接称其为“贝叶斯估计”，当然贝叶斯估计不仅仅是我们今天说的这个，但是整体的思想是相通的。 极大后验概率估计是假设参数$\theta$具有一个先验概率，然后根据训练集$D=\{x_1,x_2,x_3,...,x_n\}$ 使得参数的可能性达到最大。

MAP 估计的核心思想就是将待估参数$\theta$看成是一个随机变量、从而引入了极大似然估计里面没有引入的、参数$\theta$ 的先验分布。MAP 估计$\theta_{MAP}$的定义为

$$

\theta_{MAP} = \arg\max_{\theta} P(\theta | D) 
= \arg \max_{\theta} \frac{P(D | \theta ) P(\theta)}{P(D)}
 = \arg \max_{\theta} \frac{P(\theta) \prod_{i=1}^N P(x_i | \theta)}{P(D)}

$$

由于$P(D)$与参数$\theta$无关，所以上式可以化简为：

$$

\theta_{MAP} = \arg \max_{\theta} P(\theta) \prod_{i=1}^N P(x_i | \theta)

$$

可以看到，从形式上、极大后验概率估计只比极大似然估计多了$P(\theta)$这一项，不过它们背后的思想却相当不同。不过有意思的是， 在之后具体讨论朴素贝叶斯算法时我们会看到、朴素贝叶斯在估计参数时选用了极大似然估计法、但是在做决策时则选用了 MAP 估计。和极大似然估计相比， MAP 估计的一个显著优势在于它可以引入所谓的“先验知识”，这正是贝叶斯学派的精髓。当然这个优势同时也伴随着劣势：它要求我们对模型参数有相对较好的认知， 否则会相当大地影响到结果的合理性。

由上面的讨论可知，极大似然估计与贝叶斯估计最大的不同就在于**是否考虑了先验**，而两者**适用范围**也变成了：极大似然估计适用于数据大量，估计的参数能够较好的反映实际情况；而贝叶斯估计则在数据量较少或者比较稀疏的情况下，考虑先验来提升准确率。





### 正定矩阵

一个$n×n$的**实对称矩阵** $ M$是正定的，当且仅当对于所有的非零实系数向量$z$，都有$z^TMz > 0$。其中$z^T$表示$z$的转置。

### 雅克比矩阵
![](/img/blog_imgs/daoshu.png)














